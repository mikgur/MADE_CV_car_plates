{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as fnn\n",
    "import torchvision.models as models\n",
    "\n",
    "from detection import create_detection_model, DetectionDataset, Flip, PerspectiveTransform\n",
    "from recognition import CRNN, RecognitionDataset, beam_search, LanguageModel\n",
    "from detection_utils import PlateImageAdjuster, PlateImageExtractor\n",
    "from recognition_utils import Resize, collate_fn_recognition_test\n",
    "from classifier import ClassifierDataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    (transforms.ToPILImage(), 'image'),\n",
    "    (transforms.ToTensor(), 'image'),\n",
    "                    ]\n",
    "\n",
    "test_dataset = DetectionDataset('data', transformations, 'test')\n",
    "\n",
    "test_dataloader = data.DataLoader(\n",
    "        test_dataset, batch_size=2,\n",
    "        num_workers=4, pin_memory=True,\n",
    "        shuffle=False, drop_last=False,\n",
    "        collate_fn=DetectionDataset.collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection model Loaded\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = create_detection_model()\n",
    "\n",
    "with open('SGD_lr_3e-4_plateau_best.pth', 'rb') as fp:\n",
    "    state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('Detection model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1594 [00:00<?, ?it/s]/home/mikhail/miniconda3/envs/made_cv/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n",
      "100%|██████████| 1594/1594 [06:14<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD_MASK = 0.05\n",
    "THRESHOLD_BOX = 0.92\n",
    "\n",
    "path_test_ocr = Path('test_ocr_data')\n",
    "path_test_ocr.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "normalizer = PlateImageAdjuster()\n",
    "extractor = PlateImageExtractor()\n",
    "\n",
    "test_plates_filenames = []\n",
    "\n",
    "for i, batch in enumerate(tqdm.tqdm(test_dataloader)):\n",
    "    images = list(image.to(device) for image in batch[0])\n",
    "    filenames = list(filename['filename'] for filename in batch[1])\n",
    "    with torch.no_grad():\n",
    "        preds = model(images)\n",
    "    for pred, image_tensor, filename in zip(preds, images, filenames):\n",
    "        ps = pred['scores'].detach().cpu().numpy()\n",
    "        boxes = pred['boxes'].detach().cpu().numpy()\n",
    "        masks = (pred['masks'].detach().cpu().squeeze(1).numpy() > THRESHOLD_MASK).astype(np.uint8)\n",
    "        image = image_tensor.cpu().permute(1, 2, 0).numpy() * 255\n",
    "        sorted_triads = sorted(list(zip(ps, boxes, masks)), key = lambda x: x[1][0])\n",
    "        n = 0\n",
    "        for p, box, mask in sorted_triads:\n",
    "            if p > THRESHOLD_BOX:\n",
    "            # Too small images are useless\n",
    "                if (box[2] - box[0]) * (box[3] - box[1]) < 100:\n",
    "                    continue\n",
    "                plate_image = extractor(image, mask, box)\n",
    "                plate_image = normalizer(plate_image)\n",
    "                path = Path(filename)\n",
    "                plate_file_name = ''.join(['_'.join([path.stem, str(n)]), path.suffix])\n",
    "                cv2.imwrite(str(path_test_ocr / plate_file_name), plate_image)\n",
    "                test_plates_filenames.append(plate_file_name)\n",
    "\n",
    "                # Save bbox_image\n",
    "                bbox_image = image[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "                bbox_image_file_name = ''.join(['_'.join([path.stem, str(n), 'bbox']), path.suffix])\n",
    "                bbox_image = normalizer(bbox_image)\n",
    "                cv2.imwrite(str(path_test_ocr / bbox_image_file_name), bbox_image)\n",
    "                n += 1\n",
    "        if n == 0:\n",
    "            j = np.argmax(ps)\n",
    "            plate_image = extractor(image, masks[j], box[j])\n",
    "            plate_image = normalizer(plate_image)\n",
    "            path = Path(filename)\n",
    "            plate_file_name = ''.join(['_'.join([path.stem, str(n)]), path.suffix])\n",
    "            cv2.imwrite(str(path_test_ocr / plate_file_name), plate_image)\n",
    "            test_plates_filenames.append(plate_file_name)\n",
    "            \n",
    "            # Save bbox_image\n",
    "            bbox_image = image[int(boxes[j][1]):int(boxes[j][3]), int(boxes[j][0]):int(boxes[j][2])]\n",
    "            bbox_image_file_name = ''.join(['_'.join([path.stem, str(n), 'bbox']), path.suffix])\n",
    "            bbox_image = normalizer(bbox_image)\n",
    "            cv2.imwrite(str(path_test_ocr / bbox_image_file_name), bbox_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_test_ocr / 'test_plates_filenames.json', 'w') as f:\n",
    "    json.dump(test_plates_filenames, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "preds = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn = CRNN(rnn_bidirectional=True)\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    Resize(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "test_ocr_dataset = RecognitionDataset('test_ocr_data', transformations, crnn.alphabet, 'test')\n",
    "test_ocr_dataloader = torch.utils.data.DataLoader(test_ocr_dataset, \n",
    "                                                  batch_size=batch_size, shuffle=False,\n",
    "                                                  num_workers=num_workers, pin_memory=True, \n",
    "                                                  drop_last=False, collate_fn=collate_fn_recognition_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition model Loaded\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "with open('Recognition_model_with_generated_test.pth', 'rb') as fp:\n",
    "    state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "crnn.load_state_dict(state_dict)\n",
    "crnn.to(device)\n",
    "crnn.eval()\n",
    "print('Recognition model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [05:56<00:00,  6.47s/it]\n"
     ]
    }
   ],
   "source": [
    "filenames_list = []\n",
    "text_pred = []\n",
    "text_conf = []\n",
    "confidence = []\n",
    "max_prob = []\n",
    "min_prob = []\n",
    "\n",
    "submission_preds = {}\n",
    "lm = LanguageModel()\n",
    "\n",
    "for batch in tqdm.tqdm(test_ocr_dataloader):\n",
    "    with torch.no_grad():\n",
    "        preds = crnn(batch['image'].to(device))\n",
    "        preds_bbox = crnn(batch['image_bbox'].to(device))\n",
    "    preds = preds + preds_bbox\n",
    "    probs = fnn.softmax(preds, dim=2)\n",
    "    preds_with_confidence = [beam_search(pred, crnn.alphabet, beam_width=20, lm=lm, alpha=0.3, beta=4) for pred in probs.permute(1, 0, 2).cpu().data.numpy()]\n",
    "    texts_pred = [a[0] for a in preds_with_confidence]\n",
    "    batch_confidence = [a.item() for a in probs.permute(1, 0, 2).std(dim=2).mean(dim=1)]\n",
    "    \n",
    "    filenames = batch['file_name']\n",
    "    for filename, text, conf_score in zip(filenames, texts_pred, batch_confidence):\n",
    "        test_file_name, num = filename.stem.split('_')\n",
    "        test_file_name = ''.join(['test/', test_file_name, filename.suffix])\n",
    "        if test_file_name not in submission_preds:\n",
    "            submission_preds[test_file_name] = {}\n",
    "        submission_preds[test_file_name][int(num)] = (text, conf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRESHOLD = 0.201\n",
    "\n",
    "submission_dict = defaultdict(str)\n",
    "for key in submission_preds:\n",
    "    sorted_keys = sorted(submission_preds[key].keys())\n",
    "    if len(sorted_keys) > 1:\n",
    "        submission_dict[key] = ' '.join([submission_preds[key][k][0] \n",
    "                                             for k in sorted_keys if submission_preds[key][k][1] > CONF_THRESHOLD])\n",
    "    else:\n",
    "        submission_dict[key] = submission_preds[key][sorted_keys[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>plates_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/0.jpg</td>\n",
       "      <td>O195KC96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/1.jpg</td>\n",
       "      <td>O001OO24 O005OO29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/2.jpg</td>\n",
       "      <td>H030MB33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/3.jpg</td>\n",
       "      <td>C139AP96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/4.bmp</td>\n",
       "      <td>B955ET35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3183</th>\n",
       "      <td>test/3183.jpg</td>\n",
       "      <td>M143ME27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184</th>\n",
       "      <td>test/3184.jpg</td>\n",
       "      <td>X411AX01 P481HC93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3185</th>\n",
       "      <td>test/3185.bmp</td>\n",
       "      <td>B692KT35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>test/3186.bmp</td>\n",
       "      <td>A184XE38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>test/3187.bmp</td>\n",
       "      <td>B752HE35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_name      plates_string\n",
       "0        test/0.jpg           O195KC96\n",
       "1        test/1.jpg  O001OO24 O005OO29\n",
       "2        test/2.jpg           H030MB33\n",
       "3        test/3.jpg           C139AP96\n",
       "4        test/4.bmp           B955ET35\n",
       "...             ...                ...\n",
       "3183  test/3183.jpg           M143ME27\n",
       "3184  test/3184.jpg  X411AX01 P481HC93\n",
       "3185  test/3185.bmp           B692KT35\n",
       "3186  test/3186.bmp           A184XE38\n",
       "3187  test/3187.bmp           B752HE35\n",
       "\n",
       "[3188 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('submission.csv')\n",
    "submission['plates_string'] = submission.file_name.apply(lambda x: submission_dict[x])\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>plates_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_name, plates_string]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[submission.plates_string == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>plates_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_name, plates_string]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[submission.plates_string.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('bidir_lm_with_conf_normalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
