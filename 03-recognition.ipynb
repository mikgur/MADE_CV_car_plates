{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from Levenshtein import distance\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as fnn\n",
    "\n",
    "from detection_utils import PlateImageAdjuster, PlateImageExtractor, build_mask, get_rectangular_box\n",
    "from recognition import CRNN, RecognitionDataset, LanguageModel, beam_search\n",
    "from recognition_utils import collate_fn_recognition, decode, normalize_text, Resize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = PlateImageAdjuster()\n",
    "extractor = PlateImageExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare OCR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25633/25633 [14:53<00:00, 28.69it/s]\n"
     ]
    }
   ],
   "source": [
    "path_data = Path('data')\n",
    "path_ocr_dataset = Path('ocr_data')\n",
    "path_ocr_dataset.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plates_filename = path_data / 'train.json'\n",
    "with open(plates_filename) as f:\n",
    "    json_data = json.load(f)\n",
    "    \n",
    "for sample in tqdm.tqdm(json_data):\n",
    "    if sample['file'] == 'train/25632.bmp':\n",
    "        continue\n",
    "    file_path = path_data / sample['file']\n",
    "    image = cv2.imread(str(file_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for plate in sample['nums']:\n",
    "        box = plate['box']\n",
    "        text = plate['text']\n",
    "        mask = build_mask(box, image)\n",
    "        plate_img = extractor(image, mask, np.array(box))\n",
    "        plate_img = normalizer(plate_img)\n",
    "        text = normalize_text(text)\n",
    "        file_path = path_ocr_dataset / ''.join([text, '.png'])\n",
    "        cv2.imwrite(str(file_path), plate_img)\n",
    "        \n",
    "        # save also bboxes\n",
    "        file_path = path_ocr_dataset / ''.join([text, '_bbox.png'])\n",
    "        raw_box = get_rectangular_box(box)\n",
    "        plate_bbox = image[raw_box[1]:raw_box[3], raw_box[0]:raw_box[2], :]\n",
    "        plate_bbox = normalizer(plate_bbox)\n",
    "        cv2.imwrite(str(file_path), plate_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "crnn = CRNN(rnn_bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn.to(device)\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "optimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=1/np.sqrt(10), patience=2,\n",
    "                                                          verbose=True, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    Resize(),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "train_ocr_dataset = RecognitionDataset('ocr_data', transformations, crnn.alphabet, 'train', add_generated=True)\n",
    "val_ocr_dataset = RecognitionDataset('ocr_data', transformations, crnn.alphabet, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_ocr_dataset, \n",
    "                                               batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=num_workers, pin_memory=True, \n",
    "                                               drop_last=False, collate_fn=collate_fn_recognition)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_ocr_dataset, \n",
    "                                             batch_size=1, shuffle=False,\n",
    "                                             num_workers=num_workers, pin_memory=True, \n",
    "                                             drop_last=True, collate_fn=collate_fn_recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'Recognition_model_with_generated_test'\n",
    "writer = SummaryWriter(log_dir=f'tb_logs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1, 0.3902352593793738\n",
      "Train 1 Levenstein, 0.8286814809021498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:19<00:00, 121.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 1, 0.06148304197738804\n",
      "Valid 1 Levenstein, 0.11012658227848102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 2, 0.1106603493974582\n",
      "Train 2 Levenstein, 0.1400639364936601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:16<00:00, 145.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 2, 0.048997855991496454\n",
      "Valid 2 Levenstein, 0.08270042194092828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 3, 0.10215797124947744\n",
      "Train 3 Levenstein, 0.12761880554116278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 150.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 3, 0.04481369339382543\n",
      "Valid 3 Levenstein, 0.07383966244725738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 4, 0.09646202805536502\n",
      "Train 4 Levenstein, 0.12163575751058275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 148.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 4, 0.04216035803525025\n",
      "Valid 4 Levenstein, 0.07130801687763713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 5, 0.09259370516483591\n",
      "Train 5 Levenstein, 0.11800877904760043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 154.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 5, 0.03749326401540444\n",
      "Valid 5 Levenstein, 0.06118143459915612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 6, 0.08972874121959458\n",
      "Train 6 Levenstein, 0.11593621992589624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 153.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 6, 0.042026878098067176\n",
      "Valid 6 Levenstein, 0.0751054852320675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 7, 0.0875242827693146\n",
      "Train 7 Levenstein, 0.11485105925368319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 152.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 7, 0.04159143080556409\n",
      "Valid 7 Levenstein, 0.07215189873417721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 8, 0.08511661478709091\n",
      "Train 8 Levenstein, 0.11354104546920979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:14<00:00, 160.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     8: reducing learning rate of group 0 to 9.4868e-05.\n",
      "Valid 8, 0.03880901620359816\n",
      "Valid 8 Levenstein, 0.06413502109704641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 9, 0.07566881292366223\n",
      "Train 9 Levenstein, 0.10700075276911496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:14<00:00, 160.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 9, 0.03134773735425834\n",
      "Valid 9 Levenstein, 0.0510548523206751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 10, 0.07210314114944423\n",
      "Train 10 Levenstein, 0.1057493963182747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:16<00:00, 147.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 10, 0.031436513337845286\n",
      "Valid 10 Levenstein, 0.05063291139240506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 11, 0.0696104252833251\n",
      "Train 11 Levenstein, 0.10551476698374214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:16<00:00, 146.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 11, 0.03275709099780753\n",
      "Valid 11 Levenstein, 0.05063291139240506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 12, 0.06751998940516517\n",
      "Train 12 Levenstein, 0.10612089276461789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 150.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 12, 0.032377518696509316\n",
      "Valid 12 Levenstein, 0.05147679324894515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 13, 0.06518693025217503\n",
      "Train 13 Levenstein, 0.10663903254504395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 150.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    13: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Valid 13, 0.033466638661017206\n",
      "Valid 13 Levenstein, 0.0540084388185654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 14, 0.056936792326884915\n",
      "Train 14 Levenstein, 0.10500640342558828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:16<00:00, 146.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 14, 0.03274462418699488\n",
      "Valid 14 Levenstein, 0.049789029535864976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 15, 0.05406321141533344\n",
      "Train 15 Levenstein, 0.10554409565055871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 153.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 15, 0.033340666222697116\n",
      "Valid 15 Levenstein, 0.05021097046413502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 16, 0.052186401377142574\n",
      "Train 16 Levenstein, 0.10587648720781316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:14<00:00, 158.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 16, 0.0338942266156572\n",
      "Valid 16 Levenstein, 0.049789029535864976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 17, 0.05027586026082259\n",
      "Train 17 Levenstein, 0.1069616478800262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 157.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    17: reducing learning rate of group 0 to 9.4868e-06.\n",
      "Valid 17, 0.03399190202613328\n",
      "Valid 17 Levenstein, 0.05063291139240506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 18, 0.045147094381227625\n",
      "Train 18 Levenstein, 0.1036866134188427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 148.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 18, 0.03448477631350004\n",
      "Valid 18 Levenstein, 0.05147679324894515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:30<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 19, 0.04387383985046584\n",
      "Train 19 Levenstein, 0.10398967630928056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:15<00:00, 154.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid 19, 0.034593770058170714\n",
      "Valid 19 Levenstein, 0.052742616033755275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599/1599 [07:31<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 20, 0.042898616077305\n",
      "Train 20 Levenstein, 0.10439050142244034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2370/2370 [00:17<00:00, 137.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    20: reducing learning rate of group 0 to 3.0000e-06.\n",
      "Valid 20, 0.034873423276246596\n",
      "Valid 20 Levenstein, 0.053164556962025315\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "prev_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "for i, epoch in enumerate(range(num_epochs)):\n",
    "    epoch_losses = []\n",
    "    levensteint_losses = []\n",
    "    \n",
    "    # Если поменялась lr - загружаем лучшую модель\n",
    "    if optimizer.param_groups[0]['lr'] < prev_lr:\n",
    "        prev_lr = optimizer.param_groups[0]['lr']\n",
    "        with open(f'{experiment_name}.pth', 'rb') as fp:\n",
    "            state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "        crnn.load_state_dict(state_dict)\n",
    "        crnn.to(device)\n",
    "    \n",
    "    crnn.train()\n",
    "    for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "        log_probs = fnn.log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "        \n",
    "        texts_pred = decode(seqs_pred, crnn.alphabet)\n",
    "        texts_gt = b[\"text\"]\n",
    "        levensteint_losses.extend([distance(pred, gt) for pred, gt in zip(texts_pred, texts_gt)])\n",
    "\n",
    "        loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "    print(f'Train {i + 1}, {np.mean(epoch_losses)}')\n",
    "    print(f'Train {i + 1} Levenstein, {np.mean(levensteint_losses)}')\n",
    "    writer.add_scalar('Recognition/Train/loss', np.mean(epoch_losses), i)\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    levensteint_losses = []\n",
    "    crnn.eval()\n",
    "    for j, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = crnn(images).cpu()\n",
    "        log_probs = fnn.log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "        \n",
    "        texts_pred = decode(seqs_pred, crnn.alphabet)\n",
    "        texts_gt = b[\"text\"]\n",
    "        levensteint_losses.extend([distance(pred, gt) for pred, gt in zip(texts_pred, texts_gt)])\n",
    "\n",
    "        loss = fnn.ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                            targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                            input_lengths=seq_lens_pred,  # N\n",
    "                            target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        if best_loss > epoch_losses[-1]:\n",
    "            best_loss = epoch_losses[-1]\n",
    "            with open(f'{experiment_name}.pth', 'wb') as fp:\n",
    "                torch.save(crnn.state_dict(), fp)\n",
    "        \n",
    "    lr_scheduler.step(np.mean(levensteint_losses))\n",
    "    print(f'Valid {i + 1}, {np.mean(epoch_losses)}')\n",
    "    print(f'Valid {i + 1} Levenstein, {np.mean(levensteint_losses)}')\n",
    "    writer.add_scalar('Recognition/Valid/loss', np.mean(epoch_losses), i)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "with open(f'{experiment_name}.pth', 'rb') as fp:\n",
    "    state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "crnn = CRNN(rnn_bidirectional=True)\n",
    "crnn.load_state_dict(state_dict)\n",
    "crnn.to(device)\n",
    "crnn.eval()\n",
    "print('Model loaded!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
